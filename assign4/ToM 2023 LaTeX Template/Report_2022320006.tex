%%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{tom2023}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{csvsimple}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}



% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{COSE361(03) Final Project 2/2 Report}

\begin{document}

\twocolumn[
\icmltitle{COSE361(03) \\
           Final Project 2/2 Report}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.


\begin{icmlauthorlist}
\icmlauthor{Choi Jinhyeok, Student No. 2022320006}{ }
\end{icmlauthorlist}


\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution



\section{Introduction}
\label{submission}

The final project(2/2) is an implementing of two-agent team against opponent's team. The map is divided into 2 side, red and blue. Each agent is a pacman or ghost, depending on the agent's location. If my team is red, my agent is ghost when in the red side, and pacman in the blue side. We should eat foods on the enemy's side as much as possible, and protect foods in our side against enemy pacman.

My implementation is based on the baseline code, because the evaluation method for the successor is quite efficient. Thus, I use the baseline code for base logic, but also add some other features so that the agent can consider other important information.

\section{Methods}
Details about my implemented 3 agents

\subsection{your\_baseline1}

Main difference of my agents is that the offensive agent can be defensive under some conditions. Offensive agent can be a defensive agent when the score is high enough. The threshold is half of the number of the protecting foods. It also consider the features 'distanceToGhost', 'distanceToCapsule', 'distanceToHome'.

'distanceToGhost' is the feature about the distance to the ghost. When the ghosts are scared, the feature is 50. Otherwise, the feature is the minimum distance to the ghost, especially -9999 when the distance to ghost is 5 or less.

'distanceToCapsule' is the feature about the minimum distance to the capsule. It is same as the actual minimum distance to capsule. The weight is -10, which is more important than food.

'distanceToHome' is the feature for safe pacman returning. When pacman eat some foods, it is important to return agent's side to get the points. Thus, it was implemented to calculate the distance to the home as the feature, when the pacman ate more than limit number of food, so that the pacman do not goes far away from home. When there is no opponent's ghost, or all opponent agent is pacman, this feature will be ignored to free eat.

Defensive agent is also based on the baseline, but it will walk around near the capsule position. For this, new feature 'disDiffToCapsule' is added.

'disDiffToCapsule' is the feature about the difference of distance to capsule of agent and enemy pacman. Defending the capsule is important because eating capsule can make enemy agents scared. Thus, defensive agent will try to keep his distance to capsule less than the enemy's. First, find the minimum distance between enemy pacman and capsule with the target capsule. When the enemy's distance is large enough, just keep closer to the target capsule than the enemy. Otherwise, or when the enemy is close to the capsule, the agent's distance to the capsule become much important with higher weight.

\subsection{your\_baseline2}

This is an improved version of 'your\_baseline1'. Both offensive and defensive agent has modified with some new features or improved way to compute their features.

First, the offensive agent will become a defensive agent when the agent is on his side and the all of opponent's agent is pacman. This is the case when the protecting foods is more important than attacking.

Second, 'distanceToHome' feature has modified to include the number of foods that pacman is carrying. The new feature is multiple of the distance to the home and the number of foods carrying. It makes the returning with larger number of food is more important.

The new feature 'eatCapsule' is included for offensive agent to eat capsule. If the next position of the agent is the same as the capsule position, or it is guaranteed to eat capsule, the agent will always eat the capsule. This is because there is no other threat of the ghost, and it is more important to eat capsule than one single food.

The 'disDiffToCapsule' feature was also adjusted to consider more specific circumstances. When the opponent's distance to capsule is large enough, the feature is just the agent's distance to capsule. When the opponent is close to the capsule some extent, the distance difference is matter. If the agent is closer than the opponent's pacman, the feature is the agent's distance to capsule with the corresponding weights. If the opponent's pacman is closer than the agent, the feature is worst case value. When the opponent is much close to the capsule, the feature is square of the distance to the capsule with weight, so that the distance become more important.



\subsection{your\_baseline3}

This is an upgrade version of 'your\_baseline2', which has the best performance among the three agents. It was focused to improve offensive agent to survive from the ghosts, has some new features 'eatGhost', 'eatInvader' as well. The defensive agent is same as the 'your\_baseline2'.

Computation method for feature 'distanceToGhost' was improved. Surviving is the most important thing of the offensive agent, thus the feature has high value than other features. Large distance to the ghost is preferred, so the feature must be proportional to the distance. If the feature has extremely high value where threshold for having the feature (i.e. like a stair function), it can make the agent to stop on the threshold while the ghost is approaching. The feature 'distanceToGhost' is designed to be zero when the agent is not a pacman, so in some cases, agent will stop on the half line to remain as a pacman. This is because the smaller distance to the ghost has higher evaluation value rather than to move agent's side making feature is zero. Thus, the feature was adjusted as following equation.

\begin{equation}
    distanceToGhost =
    \left\{\begin{matrix}
\frac{-9}{x + \alpha} & (x \leq 10) \\
& \\
\frac{9}{100}x - \frac{9}{5} & (10 < x \leq 20) \\
& \\
0, & elsewhere \\
\end{matrix}\right.
\end{equation}



Where the $x$ is the minimum distance to the ghost, and the $\alpha$ is adjustment factor. This equation make the feature value function continuous and smooth, so that the above problem can be resolved. The weight of the feature is 10.

New feature 'eatGhost' is for eating scared ghost. If the next position generated by the action is same as the position of the sacred ghost, the agent will always eat the ghost. 

'eatInvader' is similar to this. When the offensive agent is offensive mode, there are some cases that the agent ignore adjacent opponent pacman, and just go to the opponent's side. Although it's purpose is to eat foods now, there was a chance to eat opponent pacman. Thus the agent was modified to eat the right beside pacman. If the offensive agent is not a pacman, and the next position is same as the opponent pacman, the agent will always eat the pacman, not changing his offensive mode.

Moreover, when the offensive agent is on the defense mode, the agent will ignore protecting capsule. The offensive agent does not have a 'disDiffToCapsule' feature, but the defensive agent only. This is because when the all agents are on the defense mode, it is efficient to disperse two agents.

This agent has higher performance among 3 agents, thus this was choosed as the 'your\_best'.

\section{Results}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|}
    \hline
        ~ & your\_best(red) \\ \hline
        ~ & $<$Average Winning Rate$>$ \\ \hline
        your\_base1 & 0.9 \\ \hline
        your\_base2 & 0.8 \\ \hline
        your\_base3 & -0.2 \\ \hline
        baseline & 1.0 \\ \hline
        Num\_Win & 3.0 \\ \hline
        Avg\_Winning\_Rate & 0.625 \\ \hline
        ~ & $<$Average Scores$>$ \\ \hline
        your\_base1 & 2.9 \\ \hline
        your\_base2 & 1.0 \\ \hline
        your\_base3 & -0.5 \\ \hline
        baesline & 4.5 \\ \hline
        Avg\_Score & 1.975 \\ \hline
    \end{tabular}
\end{table}

This is an $output.csv$ file. In this result, 'your\_best' is same as the 'your\_base3'. The result shows that 'your\_best' wins every time against 'baseline', also wins with high winning rate against 'your\_baseline1' and 'your\_baseline2'.

\section{Conclusion \& Free Discussion}

Q: What if the defensive agent also can be a offensive agent in some conditions?

A: It is my big interest, because it means fully-reflex agent. I think it is more efficient in some cases such as when all opponent agents are scared, because it is possible to eat more food in time. However, the map is not big for two food-eating pacman, thus it is not guaranteed to higher performance. Rather, the more complex circumstance is possible, so a lot of features can be required to manage it. I want to design fully-reflex agent and to evaluate which agent is better, but it is not easy due to other assignments and exams. I will try it after the semester.

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
